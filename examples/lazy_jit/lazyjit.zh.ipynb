{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0deecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.absolute()))\n",
    "import tilelang\n",
    "import torch\n",
    "import tilelang.language as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2c56d",
   "metadata": {},
   "source": [
    "# Tilelang Lazy JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e7370",
   "metadata": {},
   "source": [
    "## Tensor Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070c109",
   "metadata": {},
   "source": [
    "Tilelang Lazy JIT 将 jit 生成和调用的逻辑合并到一起\n",
    "\n",
    "函数签名的写法与 triton 相似，但做了大量增强，最主要的增强是允许对 Tensor 的标注：\n",
    "\n",
    "* 如果一个 Tensor 有复杂的 shape 约束，我们可以把它的标注移动到函数内部\n",
    "* 通过 `T.const` 或 `T.dynamic` 来建立一些 shape 变量，然后用 `T.Tensor` 标注复杂的 Tensor\n",
    "* 用 `T.empty` 来声明返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bf8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def gemm(\n",
    "    A,\n",
    "    B,\n",
    "    out_dtype: T.dtype = T.float32,\n",
    "    block_M: int = 128,\n",
    "    block_N: int = 128,\n",
    "    block_K: int = 32,\n",
    "):\n",
    "    M, N, K = T.const(\"M, N, K\")\n",
    "\n",
    "    A: T.Tensor[[M, K], T.float16]\n",
    "    B: T.Tensor[[K, N], T.float16]\n",
    "\n",
    "    C = T.empty((M, N), out_dtype)\n",
    "\n",
    "    with T.Kernel(T.ceildiv(M, block_M), T.ceildiv(N, block_N), threads=128) as (bx, by):\n",
    "        A_shared = T.alloc_shared((block_M, block_K), A.dtype)\n",
    "        B_shared = T.alloc_shared((block_K, block_N), B.dtype)\n",
    "        C_local = T.alloc_fragment((block_M, block_N), out_dtype)\n",
    "        T.clear(C_local)\n",
    "        for k in T.Pipelined(T.ceildiv(K, block_K), num_stages=3):\n",
    "            T.copy(A[bx * block_M, k * block_K], A_shared)\n",
    "            T.copy(B[k * block_K, by * block_N], B_shared)\n",
    "            T.gemm(A_shared, B_shared, C_local)\n",
    "        T.copy(C_local, C[bx * block_M, by * block_N])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f868fe",
   "metadata": {},
   "source": [
    "直接将 Tensor 作为参数调用，即可触发完整的 jit 编译运行流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee13394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 512, dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.randn(512, 256, dtype=torch.float16, device=\"cuda\")\n",
    "C = gemm(A, B)\n",
    "\n",
    "# check output is correct\n",
    "C_ref = (A @ B).float()\n",
    "torch.testing.assert_close(C, C_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6705091",
   "metadata": {},
   "source": [
    "更改调用的参数，如果编译器参数发生了变化，会触发重新编译："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8aab5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 512, dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.randn(512, 1024, dtype=torch.float16, device=\"cuda\")\n",
    "C = gemm(A, B, block_M=64, block_N=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b7391",
   "metadata": {},
   "source": [
    "你也可以手动调用 compile 函数编译 kernel\n",
    "\n",
    "1. `ker.compile` 编译 kernel\n",
    "2. `ker.get_tir` 获取 tir\n",
    "3. `ker.par_compile` 并行编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cf3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gemm.compile(A, B, block_M=64, block_N=64)\n",
    "C = kernel(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921761b5",
   "metadata": {},
   "source": [
    "## More Tensor Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539e54e",
   "metadata": {},
   "source": [
    "### 用 macro 来分离实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96ba65",
   "metadata": {},
   "source": [
    "接下来，我们会用各种方式来实现一个简单的 gemm，为了方便，我们先写一个 macro 把 gemm 的主要逻辑写出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171d4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.macro\n",
    "def gemm_impl(A, B, C, M, N, K, block_M, block_N, block_K):\n",
    "    with T.Kernel(T.ceildiv(M, block_M), T.ceildiv(N, block_N), threads=128) as (bx, by):\n",
    "        A_shared = T.alloc_shared((block_M, block_K), A.dtype)\n",
    "        B_shared = T.alloc_shared((block_K, block_N), B.dtype)\n",
    "        C_local = T.alloc_fragment((block_M, block_N), C.dtype)\n",
    "        T.clear(C_local)\n",
    "        for k in T.Pipelined(T.ceildiv(K, block_K), num_stages=3):\n",
    "            T.copy(A[bx * block_M, k * block_K], A_shared)\n",
    "            T.copy(B[k * block_K, by * block_N], B_shared)\n",
    "            T.gemm(A_shared, B_shared, C_local)\n",
    "        T.copy(C_local, C[bx * block_M, by * block_N])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a1acd",
   "metadata": {},
   "source": [
    "### 用 T.dynamic 标记动态 Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a38aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def gemm_dyn_K(A, B):\n",
    "    M, N, K = T.dynamic(\"M, N, K\")\n",
    "    A: T.Tensor[[M, K], T.float16]\n",
    "    B: T.Tensor[[K, N], T.float16]\n",
    "    C = T.empty((M, N), T.float32)\n",
    "    gemm_impl(A, B, C, M, N, K, 128, 128, 32)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6cfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 512, dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.randn(512, 256, dtype=torch.float16, device=\"cuda\")\n",
    "C = gemm_dyn_K(A, B)\n",
    "C_ref = (A @ B).float()\n",
    "torch.testing.assert_close(C, C_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee97bf7",
   "metadata": {},
   "source": [
    "### 用 T.StridedTensor 标记带 stride 的 Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dde1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def as_contingious(A):\n",
    "    M, N, dM, dN = T.dynamic(\"M, N, dM, dN\")\n",
    "    A: T.StridedTensor[[M, N], [dM, dN], T.float32]\n",
    "    B = T.empty((M, N), A.dtype)\n",
    "    block_M = 128\n",
    "    block_N = 128\n",
    "    with T.Kernel(T.ceildiv(M, block_M), T.ceildiv(N, block_N), threads=128) as (bx, by):\n",
    "        T.copy(\n",
    "            A[bx * block_M : (bx + 1) * block_M, by * block_N : (by + 1) * block_N],\n",
    "            B[bx * block_M : (bx + 1) * block_M, by * block_N : (by + 1) * block_N],\n",
    "        )\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec2c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 1024, device=\"cuda\")\n",
    "B = as_contingious(A.T)\n",
    "B_ref = A.T.contiguous()\n",
    "torch.testing.assert_close(B, B_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb20d6",
   "metadata": {},
   "source": [
    "## More Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890df0a2",
   "metadata": {},
   "source": [
    "### 直接用参数当 annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a47d42",
   "metadata": {},
   "source": [
    "可以直接把函数参数写到 annotation 里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc17af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def gemm_ptr(\n",
    "    A,\n",
    "    B,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "):\n",
    "    A: T.Tensor[[M, K], T.float16]\n",
    "    B: T.Tensor[[K, N], T.float16]\n",
    "    C = T.empty((M, N), T.float32)\n",
    "    gemm_impl(A, B, C, M, N, K, block_M=128, block_N=128, block_K=32)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e52a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 512, dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.randn(512, 256, dtype=torch.float16, device=\"cuda\")\n",
    "C = gemm_ptr(A, B, 1024, 256, 512)\n",
    "C_ref = (A @ B).float()\n",
    "torch.testing.assert_close(C, C_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19ef90",
   "metadata": {},
   "source": [
    "### 对运行时变量的 annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5f27f",
   "metadata": {},
   "source": [
    "运行时变量也是一样，如果嫌函数 annotation 太长，可以放到函数体里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1e7598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def gemm_ptr_dyn(A, B, M, N, K):\n",
    "    M: T.int32\n",
    "    N: T.int32\n",
    "    K: T.int32\n",
    "    A: T.Tensor[[M, K], T.float16]\n",
    "    B: T.Tensor[[K, N], T.float16]\n",
    "    C = T.empty((M, N), T.float32)\n",
    "    gemm_impl(A, B, C, M, N, K, block_M=128, block_N=128, block_K=32)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9a4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, 512, dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.randn(512, 256, dtype=torch.float16, device=\"cuda\")\n",
    "C = gemm_ptr_dyn(A, B, 1024, 256, 512)\n",
    "C_ref = (A @ B).float()\n",
    "torch.testing.assert_close(C, C_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81427765",
   "metadata": {},
   "source": [
    "### 常量的约束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b084b",
   "metadata": {},
   "source": [
    "`T.const` 创建的常量 annotation 只要要被直接使用一次，否则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c90dd24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constexpr variable `M` is not used in any buffer shape or stride.\n",
      "At least one **DIRECT** usage is required. Please check:\n",
      "(1) the variable is not used\n",
      "(2) all uses are indirect, e.g. M * 2, M * 3. (you can replace them with separate constexpr variables)\n",
      "Buffer shapes: {A: [M * 2, M * 3]}\n",
      "Buffer strides: {A: [M * 3, 1]}\n"
     ]
    }
   ],
   "source": [
    "@tilelang.lazy_jit\n",
    "def example_wrong_kernel(A):\n",
    "    M = T.const(\"M\")\n",
    "    A: T.Tensor[[M * 2, M * 3], T.float32]\n",
    "    with T.Kernel(1) as _:\n",
    "        A[0, 0]\n",
    "\n",
    "\n",
    "try:\n",
    "    A = torch.randn(64, 96, dtype=torch.float32, device=\"cuda\")\n",
    "    example_wrong_kernel(A)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e762b",
   "metadata": {},
   "source": [
    "### 动态维度的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e5d7a",
   "metadata": {},
   "source": [
    "如果想要 Tensor 的 annotation 类型某个参数变化，建议改成 T.ptr + T.match_buffer 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d050321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tilelang.lazy_jit\n",
    "def dyn_annot(\n",
    "    A: T.ptr,  # 1. T.ptr type annotation\n",
    "    is_2d=False,\n",
    "):\n",
    "    if is_2d:\n",
    "        M, N = T.const(\"M, N\")\n",
    "        # 2. dynamic shape annotation inside function body\n",
    "        A = T.match_buffer(A, [M, N], T.float32)\n",
    "        with T.Kernel(1) as _:\n",
    "            A[0, 0]\n",
    "    else:\n",
    "        L = T.const(\"L\")\n",
    "        A = T.match_buffer(A, [L], T.float32)\n",
    "        with T.Kernel(1) as _:\n",
    "            A[0]\n",
    "\n",
    "\n",
    "A = torch.randn(64, 96, dtype=torch.float32, device=\"cuda\")\n",
    "dyn_annot(A, is_2d=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f1bb3",
   "metadata": {},
   "source": [
    "### 带默认参数的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc9917",
   "metadata": {},
   "source": [
    "类似 `T.float32` 标注的标量可以带默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ec86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def add_one(X, data: T.float32 = 1):\n",
    "    M, N = T.const(\"M, N\")\n",
    "    X: T.Tensor[[M, N], T.float32]\n",
    "    Y = T.empty((M, N), T.float32)\n",
    "    with T.Kernel(T.ceildiv(M, 128), threads=128) as bx:\n",
    "        for i, j in T.Parallel(128, N):\n",
    "            Y[bx * 128 + i, j] = X[bx * 128 + i, j] + data\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d49e1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1024, 1024, dtype=torch.float32, device=\"cuda\")\n",
    "Y = add_one(X)\n",
    "torch.testing.assert_close(Y, X + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02baedc",
   "metadata": {},
   "source": [
    "## 参数匹配的 Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a2972",
   "metadata": {},
   "source": [
    "LazyJIT overhead 很小，每个 constant 添加约 200ns 的 overhead\n",
    "* 200ns 大约是从 torch.Tensor 的 shape/stride 中拿参数的 ffi call 的代价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc676e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel call    : 7.68 us\n",
      "Parse cache key: 0.41 us\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "A = torch.randn(128, 128, dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.randn(128, 128, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "\n",
    "@tilelang.lazy_jit\n",
    "def dummy_kernel(A, B):\n",
    "    M, N = T.const(\"M, N\")\n",
    "    A: T.Tensor[[M, N], T.float16]\n",
    "    B: T.Tensor[[M, N], T.float16]\n",
    "    with T.Kernel(1) as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "# compile it first\n",
    "dummy_kernel(A, B)\n",
    "\n",
    "\n",
    "def eval_overhead(f):\n",
    "    start = time.perf_counter_ns()\n",
    "    for _ in range(10000):\n",
    "        f()\n",
    "    stop = time.perf_counter_ns()\n",
    "    return (stop - start) / 10000 / 1000\n",
    "\n",
    "\n",
    "kernel_call_overhead = eval_overhead(lambda: dummy_kernel(A, B))\n",
    "parse_cache_key_overhead = eval_overhead(lambda: dummy_kernel.parse_cache_key(A, B))\n",
    "\n",
    "print(f\"Kernel call    : {kernel_call_overhead:.2f} us\")\n",
    "print(f\"Parse cache key: {parse_cache_key_overhead:.2f} us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39166cb4",
   "metadata": {},
   "source": [
    "## 编译与并行编译"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fbe08",
   "metadata": {},
   "source": [
    "lazyjit 和原来的 jit 都支持并行编译\n",
    "\n",
    "为了防止 torch.tensor 白白浪费内存，可以使用 T.Tensor 来创建 placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7222e57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4e4eb3cd4445bda6e8693da31ef3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elaborating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61c2946f55547c688e629851d4e8106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel Compiling:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<tilelang.jit.kernel.JITKernel at 0x7ef9f7de7d70>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7de52b0>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7e34b30>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7e34530>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7de6900>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7e344a0>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7e347a0>,\n",
       " <tilelang.jit.kernel.JITKernel at 0x7ef9f7fb25d0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def get_configs():\n",
    "    return [\n",
    "        {\n",
    "            \"A\": T.Tensor((1024, 1024), T.float32),\n",
    "            \"B\": T.Tensor((1024, 1024), T.float32),\n",
    "            \"block_M\": block_M,\n",
    "            \"block_N\": block_N,\n",
    "            \"block_K\": block_K,\n",
    "        }\n",
    "        for block_M, block_N, block_K in product([32, 64], repeat=3)\n",
    "    ]\n",
    "\n",
    "\n",
    "gemm.par_compile(get_configs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160d2cc",
   "metadata": {},
   "source": [
    "## 更便利的 Macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44afc4",
   "metadata": {},
   "source": [
    "tilelang 的 macro 现在已经升级：\n",
    "\n",
    "1. 允许用 `T.Ref` 作为 annotation，这类似与 C++ 的引用传递\n",
    "2. 允许返回多个值\n",
    "3. 允许嵌套，递归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79575972",
   "metadata": {},
   "source": [
    "### T.Ref 传递引用\n",
    "\n",
    "T.Ref 传递的引用可以 var 也可以是 Buffer 的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90eaa6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# from tvm.script import tir as T\n",
       "\n",
       "@T.prim_func\n",
       "def foo(x_handle: T.handle):\n",
       "    x = T.match_buffer(x_handle, (2,), strides=(1,))\n",
       "    # with T.block(\"root\"):\n",
       "    bx = T.launch_thread(\"blockIdx.x\", 1)\n",
       "    tx = T.launch_thread(\"threadIdx.x\", 128)\n",
       "    ty = T.launch_thread(\"threadIdx.y\", 1)\n",
       "    tz = T.launch_thread(\"threadIdx.z\", 1)\n",
       "    with T.block(\"tilelang_root\"):\n",
       "        T.reads()\n",
       "        idx = T.Buffer((1,), \"int32\", scope=\"local.var\")\n",
       "        T.writes(x[T.min(1, idx[0]):T.min(1, idx[0]) + (T.max(1, idx[0]) + 1 - T.min(1, idx[0]))])\n",
       "        T.block_attr({\"tl.local_var_init\": {idx.data: 0}})\n",
       "        idx = T.alloc_buffer((1,), \"int32\", data=idx.data, scope=\"local.var\")\n",
       "        x[1] = T.float32(1.0)\n",
       "        _tmp: T.int32 = idx[0]\n",
       "        x[_tmp] = T.float32(1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@T.macro\n",
    "def macro_with_ref(x: T.Ref):\n",
    "    x = 1  # noqa: F841\n",
    "\n",
    "\n",
    "@T.prim_func\n",
    "def foo(x: T.Tensor((2,))):\n",
    "    with T.Kernel(1) as _:\n",
    "        # 支持常量 index\n",
    "        macro_with_ref(x[1])\n",
    "\n",
    "        # 也支持变量 index\n",
    "        idx = T.alloc_var(T.int32, 0)\n",
    "        macro_with_ref(x[idx])\n",
    "\n",
    "\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb447a2",
   "metadata": {},
   "source": [
    "### 当作参数传递\n",
    "\n",
    "你可以把 macro 当做参数传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc7bb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.lazy_jit\n",
    "def element_wise(A, fn):\n",
    "    N = T.dynamic(\"N\")\n",
    "    A: T.Tensor[[N], T.float32]\n",
    "    B = T.empty((N,), dtype=A.dtype)\n",
    "    block_N = 128\n",
    "    with T.Kernel(T.ceildiv(N, block_N), threads=128) as bx:\n",
    "        for i in T.Parallel(block_N):\n",
    "            idx = bx * block_N + i\n",
    "            B[idx] = fn(A[idx])\n",
    "    return B\n",
    "\n",
    "\n",
    "@T.macro\n",
    "def add_one(x):\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a89fdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1024, device=\"cuda\")\n",
    "B = element_wise(A, add_one)\n",
    "B_ref = A + 1\n",
    "torch.testing.assert_close(B, B_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e403a",
   "metadata": {},
   "source": [
    "### Macro 递归\n",
    "\n",
    "虽然不知道有没有这种需求，但 macro 是可以递归的，终止条件要求编译期间确定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7703cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.macro\n",
    "def n31(x, var: T.Ref):\n",
    "    if x == 1:\n",
    "        pass\n",
    "    elif x % 2 == 0:\n",
    "        var = var // 2\n",
    "        n31(x // 2, var)\n",
    "    else:\n",
    "        var = var * 3 + 1\n",
    "        n31(x * 3 + 1, var)\n",
    "\n",
    "\n",
    "@tilelang.lazy_jit\n",
    "def foo(A: T.Tensor[[1], T.int32], n: int):\n",
    "    with T.Kernel(1) as _:\n",
    "        n31(n, A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "542ddd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([100], dtype=torch.int32, device=\"cuda\")\n",
    "foo(A, 5)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30c2d2",
   "metadata": {},
   "source": [
    "### Macro 返回多个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a2388f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# from tvm.script import tir as T\n",
       "\n",
       "@T.prim_func\n",
       "def foo():\n",
       "    # with T.block(\"root\"):\n",
       "    x = T.launch_thread(\"blockIdx.x\", 32)\n",
       "    tx = T.launch_thread(\"threadIdx.x\", 128)\n",
       "    ty = T.launch_thread(\"threadIdx.y\", 1)\n",
       "    tz = T.launch_thread(\"threadIdx.z\", 1)\n",
       "    with T.block(\"tilelang_root\"):\n",
       "        T.reads()\n",
       "        T.writes()\n",
       "        s: T.int32 = T.sin(x)\n",
       "        c: T.int32 = T.cos(x)\n",
       "        a: T.int32 = s + c\n",
       "        b: T.int32 = s - c\n",
       "        T.evaluate(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@T.macro\n",
    "def sincos(x):\n",
    "    return T.sin(x), T.cos(x)\n",
    "\n",
    "\n",
    "@T.prim_func\n",
    "def foo():\n",
    "    with T.Kernel(32) as x:\n",
    "        s, c = sincos(x)\n",
    "        a = s + c  # noqa: F841\n",
    "        b = s - c  # noqa: F841\n",
    "\n",
    "\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83fea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tilelang-dev_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
